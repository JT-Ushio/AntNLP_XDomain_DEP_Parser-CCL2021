[IO]
# Data IO
data_dir = ../data
DOMAIN = PB
# Path to dataset
TRAIN = ${data_dir}/X${DOMAIN}-Full.train
# Path to trainset
DEV = ${data_dir}/X${DOMAIN}.dev
# Path to devset
TEST = ${data_dir}/X${DOMAIN}.test
# Path to testset
wordvec_dir = ../data
GLOVE = ${wordvec_dir}/giga.300.txt.gz
# Path to glove giga.100.txt.gz   glove.gz
exp_name = default
ckpt_dir = ../ckpts/${exp_name}
PRED_TRAIN = ${ckpt_dir}/train.pred
PRED_DEV = ${ckpt_dir}/dev.pred
# Path to predicted devset
PRED_TEST = ${ckpt_dir}/test.pred
# Path to predicted testset
LOG = ${ckpt_dir}/${exp_name}.log
# Path to log file
MIN_COUNT = 2
# Minimum occurrences of word vocabulary
MIN_PROB = 0.7
# Minimum occurrences of word vocabulary
LAST = ${ckpt_dir}/last.pt
# path to last checkpoint
BEST = ${ckpt_dir}/best.pt
# path to best checkpoint

[Train]
# Training setup
SEED = 666
# Set random seed
N_EPOCH = 1000
# #Epoch for training & testing
N_BATCH = 200
# True Batch size for training & testing
STEP_UPDATE = 1
# Step of update for training
STEP_VALID = 5
# Step of validate for training
N_WORKER = 0
# #Worker for data loader
IS_RESUME = False
# Continue training
LAMBDA = [0.5, 1.0]
# Lambda1 and Lambda2 in arc loss

[Opt]
# Optimizer setup
LR = 0.0012
# Learning rate in Adam
BETAS = [0.9, 0.98]
# Beta1 and Beta2 in Adam
EPS = 1e-12
# EPS in Adam
LR_DECAY = 0.75
# Decay rate of LR
LR_WARM = 400
# Warm up step of LR
LR_ANNEAL = 10000
# Anneal step of LR
LR_DOUBLE = 50400
# Double Anneal step of LR
CLIP = 5.0
# Gradient clipping

[NN]
# Network setup
IS_FIX_GLOVE = True
# Fix GLOVE emb
D_TAG = 50
# Dimension of tag embedding
D_CHAR = 32
# Dimension of char embedding
D_CHARNN_HID = 25
# Dimension of charlstm hidden vector
N_CHARNN_LAYER = 1
# Number of charlstm layers
EMB_DROP = 0.33
# Dropout rate of embedding representation
D_ARC = 512
# Dimension of ARC_MLP vector
D_REL = 128
# Dimension of REL_MLP vector
MLP_DROP = 0.33
# Dropout rate of MLP representation
MODEL_TYPE = Xformer-abs
# Type of encoder: Xformer-abs/Xformer-rel/RNN.
GNN_ATTN_DROP = 0.2
# Dropout rate of GNN attention map
N_GNN_LAYER = 1
# Number of GNN layers

[RNN]
# ENC=RNN
D_RNN_HID = 400
# Dimension of RNN hidden vector
N_RNN_LAYER = 3
# Number of RNN layers
RNN_DROP = 0.4
# Dropout rate of RNN representation

[Xformer]
# ENC=Xformer
D_MODEL = 400
# Dimension of xformer hidden vector
N_HEAD = 8
# Dropout rate of RNN representation
D_FF = 800
# Dimension of xformer Feed Forward
D_K = 400
# Dimension of xformer key projection
D_V = 400
# Dimension of xformer value projection
XFMR_ATTN_DROP = 0.3
# Dropout rate of Xformer attention matrix
XFMR_RES_DROP = 0.3
# Dropout rate of Xformer res-net representation
XFMR_FFN_DROP = 0.3
# Dropout rate of Xformer feedforward representation
N_XFMR_LAYER = 8
# Number of Xformer layers
MAX_REL_DIST = 15
# Distance of Xformer rel position
USE_NEG_DIST = True
# Whether to use negative rel distance
HAS_MHA_BIAS = True
# Whether it has biases in MHA
PE_TYPE = add
# Type of PE (sincos/random, cat/add).
PE_DROP = 0.33
# Dropout rate of PE
N_PE = 200
# Maximum number of PE
D_PE = 400
# Dimension of PE