import copy
import math
from typing import Optional, Any, Tuple
import warnings

import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from .dropout import SharedDropout


class ExternalAttention(nn.Module):
    r"""Allows the model to jointly attend to information
    from different representation subspaces.
    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_
    .. math::
        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O
    where :math:`head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.
    Args:
        embed_dim: total dimension of the model.
        n_heads: parallel attention heads.
        dropout: a Dropout layer on attn_output_weights. Default: 0.0.
        bias: add bias as module parameter. Default: True.
        add_bias_kv: add bias to the key and value sequences at dim=0.
        add_zero_attn: add a new batch of zeros to the key and
                       value sequences at dim=1.
        kdim: total number of features in key. Default: None.
        vdim: total number of features in value. Default: None.
        batch_first: If ``True``, then the input and output tensors are provided
            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set
    to :attr:`embed_dim` such that query, key, and value have the same
    number of features.
    Examples::
        >>> multihead_attn = nn.MultiheadAttention(embed_dim, n_heads)
        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_k: int=512,
        d_v: int=512,
        max_dist: int=10,
        p_attn_drop: float=0.,
        bias: bool=True,
        use_neg_dist:bool=False,
    ) -> None:
        super(ExternalAttention, self).__init__()
        self.d_model = d_model
        self.d_k = d_k if d_k is not None else d_model
        self.d_v = d_v if d_v is not None else d_model

        self.n_heads = n_heads
        assert self.d_k%n_heads==0, "d_k must be divisible by n_heads"
        assert self.d_v%n_heads==0, "d_v must be divisible by n_heads"
        self.scale = float(self.d_k//n_heads) ** -0.5

        # embed_dim -> self.d_k
        self.x2q = nn.Linear(d_model, self.d_k, bias)
        self.m_k = nn.Parameter(torch.Tensor(self.d_k//n_heads, 5))
        self.m_v = nn.Parameter(torch.Tensor(self.d_v//n_heads, 5))

        self.v2o = nn.Linear(self.d_v, d_model, bias=bias)
        self.bias = bias
        self.dropout = nn.Dropout(p=p_attn_drop)
        if max_dist>0:
            self.rel_k = nn.Embedding(max_dist*2+1, self.d_k//n_heads)
            self.rel_v = nn.Embedding(max_dist*2+1, self.d_v//n_heads)
            dist_x = torch.arange(0, 200).unsqueeze(0)
            dist_y = torch.arange(0, 200).unsqueeze(1)
            rel_dist = dist_x - dist_y
            if not use_neg_dist:
                rel_dist = torch.abs(rel_dist)
            rel_dist = torch.clamp(rel_dist, min=-max_dist, max=max_dist)
            rel_dist += max_dist
            self.rel_dist = rel_dist.cuda()

        self.max_dist, self.use_neg_dist = max_dist, use_neg_dist

        self._reset_parameters()

    def _reset_parameters(self):
        nn.init.orthogonal_(self.x2q.weight)
        nn.init.orthogonal_(self.m_k)
        nn.init.orthogonal_(self.m_v)
        if self.bias:
            nn.init.zeros_(self.x2q.bias)

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if '_qkv_same_embed_dim' not in state:
            state['_qkv_same_embed_dim'] = True
        super(ExternalAttention, self).__setstate__(state)

    def forward(
        self,
        query: Tensor,
        key_padding_mask: Optional[Tensor] = None,
        attn_mask: Optional[Tensor] = None,
        need_weights: bool = False) -> Tuple[Tensor, Optional[Tensor]]:
        r"""
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. When given a binary mask and a value is True,
                the corresponding value on the attention layer will be ignored. When given
                a byte mask and a value is non-zero, the corresponding value on the attention
                layer will be ignored
            need_weights: output attn_output_weights.
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
        Shapes for inputs:
            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.
            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.
            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.
            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a ByteTensor is provided, the non-zero positions will be ignored while the position
              with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the
              source sequence length.
              If a 3D mask: :math:`(N\cdot\text{num\_heads}, L, S)` where N is the batch size, L is the target sequence
              length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend
              the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
              while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
              is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
        Shapes for outputs:
            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.
            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
              L is the target sequence length, S is the source sequence length.
        """
        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.d_model
        # allow MHA to have different sizes for the feature dimension
        q = self.x2q(query)
        q = q * self.scale

        if attn_mask is not None:
            assert (
                attn_mask.dtype == torch.float32
                or attn_mask.dtype == torch.float64
                or attn_mask.dtype == torch.float16
                or attn_mask.dtype == torch.uint8
                or attn_mask.dtype == torch.bool
            ), "Only float, byte, and bool types are supported for attn_mask, not {}".format(attn_mask.dtype)
            if attn_mask.dtype == torch.uint8:
                warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
                attn_mask = attn_mask.to(torch.bool)

            if attn_mask.dim() == 2:
                attn_mask = attn_mask.unsqueeze(0)
                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:
                    raise RuntimeError("The size of the 2D attn_mask is not correct.")
            elif attn_mask.dim() == 3:
                if list(attn_mask.size()) != [bsz*n_heads, query.size(0), key.size(0)]:
                    raise RuntimeError("The size of the 3D attn_mask is not correct.")
            else:
                raise RuntimeError("attn_mask's dimension {} is not supported".format(attn_mask.dim()))

        n_heads = self.n_heads
        dk_head = self.d_k // n_heads
        dv_head = self.d_v // n_heads
        q = q.contiguous().view(-1, bsz*n_heads, dk_head).transpose(0, 1)
        k = self.m_k.transpose(0, 1).unsqueeze(0).expand(q.size(0), -1, -1)
        v = self.m_v.transpose(0, 1).unsqueeze(0).expand(q.size(0), -1, -1)
        src_len = k.size(1)

        # convert ByteTensor key_padding_mask to bool
        if key_padding_mask is not None and key_padding_mask.dtype==torch.uint8:
            warnings.warn("Byte tensor for key_padding_mask in "
                "nn.MultiheadAttention is deprecated. Use bool tensor instead.")
            key_padding_mask = key_padding_mask.to(torch.bool)
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == tgt_len
        attn_output_weights = torch.bmm(q, k.transpose(1, 2))

        assert list(attn_output_weights.size()) == [bsz * n_heads, tgt_len, src_len]
        if self.max_dist > 0:
            rel_dist = self.rel_dist[:tgt_len, :tgt_len]
            # [tgt_len, tgt_len, d_k]
            k_dist = self.rel_k(rel_dist)
            add_term = torch.matmul(q.transpose(0, 1), k_dist.transpose(1, 2))
            attn_output_weights += add_term.transpose(0, 1)

        if attn_mask is not None:
            if attn_mask.dtype == torch.bool:
                attn_output_weights.masked_fill_(attn_mask, float("-inf"))
            else:
                attn_output_weights += attn_mask

        if key_padding_mask is not None:
            attn_output_weights = attn_output_weights.view(bsz, n_heads, tgt_len, src_len)
            attn_output_weights = attn_output_weights.masked_fill(
                key_padding_mask.unsqueeze(1).unsqueeze(3),
                float("-inf"),
            )
            attn_output_weights = attn_output_weights.view(bsz * n_heads, tgt_len, src_len)

        attn_output_weights = F.softmax(attn_output_weights, dim=-2)
        attn_output_weights = F.normalize(attn_output_weights, p=1, dim=-1)
        attn_output_weights = self.dropout(attn_output_weights)

        attn_output = torch.bmm(attn_output_weights, v)
        assert list(attn_output.size()) == [bsz * n_heads, tgt_len, dk_head]
        if self.max_dist > 0:
            v_dist = self.rel_v(rel_dist)
            add_term = torch.matmul(attn_output_weights.transpose(0, 1), v_dist)
            attn_output += add_term.transpose(0, 1)
        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.d_v)
        attn_output = self.v2o(attn_output)
        if need_weights:
            # average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, n_heads, tgt_len, src_len)
            return attn_output, attn_output_weights.sum(dim=1) / n_heads
        else:
            return attn_output, None


class MultiheadAttention(nn.Module):
    r"""Allows the model to jointly attend to information
    from different representation subspaces.
    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_
    .. math::
        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O
    where :math:`head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.
    Args:
        embed_dim: total dimension of the model.
        n_heads: parallel attention heads.
        dropout: a Dropout layer on attn_output_weights. Default: 0.0.
        bias: add bias as module parameter. Default: True.
        add_bias_kv: add bias to the key and value sequences at dim=0.
        add_zero_attn: add a new batch of zeros to the key and
                       value sequences at dim=1.
        kdim: total number of features in key. Default: None.
        vdim: total number of features in value. Default: None.
        batch_first: If ``True``, then the input and output tensors are provided
            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set
    to :attr:`embed_dim` such that query, key, and value have the same
    number of features.
    Examples::
        >>> multihead_attn = nn.MultiheadAttention(embed_dim, n_heads)
        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_k: int=512,
        d_v: int=512,
        max_dist: int=10,
        p_attn_drop: float=0.,
        bias: bool=True,
        use_neg_dist:bool=False,
    ) -> None:
        super(MultiheadAttention, self).__init__()
        self.d_model = d_model
        self.d_k = d_k if d_k is not None else d_model
        self.d_v = d_v if d_v is not None else d_model

        self.n_heads = n_heads
        assert self.d_k%n_heads==0, "d_k must be divisible by n_heads"
        assert self.d_v%n_heads==0, "d_v must be divisible by n_heads"
        self.scale = float(self.d_k//n_heads) ** -0.5

        # embed_dim -> self.d_k
        self.use_separate_proj_weight = self.d_k != self.d_v
        if self.use_separate_proj_weight:
            self.x2qk = nn.Linear(d_model, 2*self.d_k, bias)
            self.x2v = nn.Linear(d_model, self.d_v, bias)
        else:
            self.x2qkv = nn.Linear(d_model, 2*self.d_k+self.d_v, bias)

        self.v2o = nn.Linear(self.d_v, d_model, bias=bias)
        self.bias = bias
        self.dropout = nn.Dropout(p=p_attn_drop)
        if max_dist>0:
            self.rel_k = nn.Embedding(max_dist*2+1, self.d_k//n_heads)
            self.rel_v = nn.Embedding(max_dist*2+1, self.d_v//n_heads)
            dist_x = torch.arange(0, 200).unsqueeze(0)
            dist_y = torch.arange(0, 200).unsqueeze(1)
            rel_dist = dist_x - dist_y
            if not use_neg_dist:
                rel_dist = torch.abs(rel_dist)
            rel_dist = torch.clamp(rel_dist, min=-max_dist, max=max_dist)
            rel_dist += max_dist
            self.rel_dist = rel_dist.cuda()

        self.max_dist, self.use_neg_dist = max_dist, use_neg_dist

        self._reset_parameters()

    def _reset_parameters(self):
        if self.use_separate_proj_weight:
            nn.init.xavier_uniform_(self.x2qk.weight)
            # nn.init.orthogonal_(self.x2qk.weight)
            nn.init.xavier_uniform_(self.x2v.weight)
            # nn.init.orthogonal_(self.x2v.weight)
            if self.bias:
                nn.init.zeros_(self.x2qk.bias)
                nn.init.zeros_(self.x2v.bias)
        else:
            nn.init.xavier_uniform_(self.x2qkv.weight)
            # nn.init.orthogonal_(self.x2qkv.weight)
            if self.bias:
                nn.init.zeros_(self.x2qkv.bias)
        if self.bias:
            nn.init.zeros_(self.v2o.bias)

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if '_qkv_same_embed_dim' not in state:
            state['_qkv_same_embed_dim'] = True
        super(MultiheadAttention, self).__setstate__(state)

    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        key_padding_mask: Optional[Tensor] = None,
        attn_mask: Optional[Tensor] = None,
        need_weights: bool = False) -> Tuple[Tensor, Optional[Tensor]]:
        r"""
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. When given a binary mask and a value is True,
                the corresponding value on the attention layer will be ignored. When given
                a byte mask and a value is non-zero, the corresponding value on the attention
                layer will be ignored
            need_weights: output attn_output_weights.
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
        Shapes for inputs:
            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.
            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.
            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.
            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a ByteTensor is provided, the non-zero positions will be ignored while the position
              with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: if a 2D mask: :math:`(L, S)` where L is the target sequence length, S is the
              source sequence length.
              If a 3D mask: :math:`(N\cdot\text{num\_heads}, L, S)` where N is the batch size, L is the target sequence
              length, S is the source sequence length. ``attn_mask`` ensure that position i is allowed to attend
              the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
              while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
              is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
        Shapes for outputs:
            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.
            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
              L is the target sequence length, S is the source sequence length.
        """
        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.d_model
        # allow MHA to have different sizes for the feature dimension
        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)
        same_q_as_k = query is key or torch.equal(query, key)
        same_k_as_v = key is value or torch.equal(key, value)
        if self.use_separate_proj_weight:
            v = self.x2v(value)
            if same_q_as_k:
                q, k = self.x2qk(key).split(self.d_k, dim=-1)
            else:
                wq, wk = self.x2qk.weight.split(self.d_k)
                if self.bias:
                    bq, bk = self.x2qk.bias.split(self.d_k)
                else:
                    bq, bk = None, None
                q = F.linear(query, wq, bq)
                k = F.linear(key, wq, bq)
        else:
            if same_q_as_k and same_k_as_v:
                q, k, v = self.x2qkv(key).split(self.d_k, dim=-1)
            else:
                if same_q_as_k:
                    wqk, wv = self.x2qkv.weight.split([2*self.d_k, self.d_v])
                    bqk, bv = self.x2qkv.bias.split([2*self.d_k, self.d_v])
                    q, k = F.linear(key, wqk, bqk).split(self.d_k, dim=-1)
                    v = F.linear(value, wv, bv)
                else:
                    wq, wk, wv = self.x2qkv.weight.split(self.d_k)
                    bq, bk, bv = self.x2qkv.bias.split(self.d_k)
                    q = F.linear(query, wq, bq)
                    k = F.linear(key, wk, bk)
                    v = F.linear(value, wv, bv)
        q = q * self.scale

        if attn_mask is not None:
            assert (
                attn_mask.dtype == torch.float32
                or attn_mask.dtype == torch.float64
                or attn_mask.dtype == torch.float16
                or attn_mask.dtype == torch.uint8
                or attn_mask.dtype == torch.bool
            ), "Only float, byte, and bool types are supported for attn_mask, not {}".format(attn_mask.dtype)
            if attn_mask.dtype == torch.uint8:
                warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
                attn_mask = attn_mask.to(torch.bool)

            if attn_mask.dim() == 2:
                attn_mask = attn_mask.unsqueeze(0)
                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:
                    raise RuntimeError("The size of the 2D attn_mask is not correct.")
            elif attn_mask.dim() == 3:
                if list(attn_mask.size()) != [bsz*n_heads, query.size(0), key.size(0)]:
                    raise RuntimeError("The size of the 3D attn_mask is not correct.")
            else:
                raise RuntimeError("attn_mask's dimension {} is not supported".format(attn_mask.dim()))

        n_heads = self.n_heads
        dk_head = self.d_k // n_heads
        dv_head = self.d_v // n_heads
        q = q.contiguous().view(-1, bsz*n_heads, dk_head).transpose(0, 1)
        k = k.contiguous().view(-1, bsz*n_heads, dk_head).transpose(0, 1)
        v = v.contiguous().view(-1, bsz*n_heads, dv_head).transpose(0, 1)
        src_len = k.size(1)

        # convert ByteTensor key_padding_mask to bool
        if key_padding_mask is not None and key_padding_mask.dtype==torch.uint8:
            warnings.warn("Byte tensor for key_padding_mask in "
                "nn.MultiheadAttention is deprecated. Use bool tensor instead.")
            key_padding_mask = key_padding_mask.to(torch.bool)
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == src_len

        attn_output_weights = torch.bmm(q, k.transpose(1, 2))
        assert list(attn_output_weights.size()) == [bsz * n_heads, tgt_len, src_len]
        if self.max_dist > 0:
            rel_dist = self.rel_dist[:tgt_len, :tgt_len]
            # [tgt_len, tgt_len, d_k]
            k_dist = self.rel_k(rel_dist)
            add_term = torch.matmul(q.transpose(0, 1), k_dist.transpose(1, 2))
            attn_output_weights += add_term.transpose(0, 1)

        if attn_mask is not None:
            if attn_mask.dtype == torch.bool:
                attn_output_weights.masked_fill_(attn_mask, float("-inf"))
            else:
                attn_output_weights += attn_mask

        if key_padding_mask is not None:
            attn_output_weights = attn_output_weights.view(bsz, n_heads, tgt_len, src_len)
            attn_output_weights = attn_output_weights.masked_fill(
                key_padding_mask.unsqueeze(1).unsqueeze(2),
                float("-inf"),
            )
            attn_output_weights = attn_output_weights.view(bsz * n_heads, tgt_len, src_len)

        attn_output_weights = F.softmax(attn_output_weights, dim=-1)
        # attn_output_weights = F.normalize(attn_output_weights, p=1, dim=-2)
        attn_output_weights = self.dropout(attn_output_weights)

        attn_output = torch.bmm(attn_output_weights, v)
        assert list(attn_output.size()) == [bsz * n_heads, tgt_len, dk_head]
        if self.max_dist > 0:
            v_dist = self.rel_v(rel_dist)
            add_term = torch.matmul(attn_output_weights.transpose(0, 1), v_dist)
            attn_output += add_term.transpose(0, 1)
        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.d_v)
        attn_output = self.v2o(attn_output)
        if need_weights:
            # average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, n_heads, tgt_len, src_len)
            return attn_output, attn_output_weights.sum(dim=1) / n_heads
        else:
            return attn_output, None


class TransformerEncoder(nn.Module):
    r"""TransformerEncoder is a stack of N encoder layers

    Args:
        encoder_layer: an instance of the TransformerEncoderLayer() class (required).
        n_layers: the number of sub-encoder-layers in the encoder (required).
        norm: the layer normalization component (optional).

    Examples::
        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, n_heads=8)
        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, n_layers=6)
        >>> src = torch.rand(10, 32, 512)
        >>> out = transformer_encoder(src)
    """
    __constants__ = ['norm']

    def __init__(self, encoder_layer, n_layers:int, norm=None):
        super(TransformerEncoder, self).__init__()
        self.layers = _get_clones(encoder_layer, n_layers)
        self.n_layers = n_layers
        self.norm = norm

    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
        r"""Pass the input through the encoder layers in turn.

        Args:
            src: the sequence to the encoder (required).
            mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        output = src

        for mod in self.layers:
            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)

        if self.norm is not None:
            output = self.norm(output)

        return output


class TransformerEncoderLayer(nn.Module):
    r"""TransformerEncoderLayer is made up of self-attn and feedforward network.
    This standard encoder layer is based on the paper "Attention Is All You Need".
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
    in a different way during application.

    Args:
        d_model: the number of expected features in the input (required).
        n_heads: the number of heads in the multiheadattention models (required).
        d_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of intermediate layer, relu or gelu (default=relu).

    Examples::
        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, n_heads=8)
        >>> src = torch.rand(10, 32, 512)
        >>> out = encoder_layer(src)
    """

    def __init__(
        self,
        d_model: int,
        d_k: int,
        d_v: int,
        d_feedforward: int=2048,
        n_heads: int=8,
        max_dist: int=0,
        p_attn_drop: float=0.1,
        p_res_drop: float=0.1,
        p_ffn_drop: float=0.1,
        bias: bool=False,
        use_neg_dist: bool=False,
        activation: str="relu"):
        super(TransformerEncoderLayer, self).__init__()
        # , use_separate_proj_weight=(True if d_k or d_v else False)
        self.MHA = MultiheadAttention(d_model, n_heads, d_k, d_v, max_dist,
                                      p_attn_drop, bias, use_neg_dist)
        # self.MHA = ExternalAttention(d_model, n_heads, d_k, d_v, max_dist,
                                     # p_attn_drop, bias, use_neg_dist)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, d_feedforward)
        self.dropout = nn.Dropout(p_ffn_drop)
        self.linear2 = nn.Linear(d_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(p_res_drop)
        self.dropout2 = nn.Dropout(p_res_drop)

        self.activation = _get_activation_fn(activation)

    def __setstate__(self, state):
        if 'activation' not in state:
            state['activation'] = F.relu
        super(TransformerEncoderLayer, self).__setstate__(state)

    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
        r"""Pass the input through the encoder layer.

        Args:
            src: the sequence to the encoder layer (required).
            src_mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        src2, _ = self.MHA(src, src, src, src_key_padding_mask, src_mask)
        # src2, _ = self.MHA(src, src_key_padding_mask, src_mask)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src


def _get_activation_fn(activation):
    if activation == "relu":
        return F.relu
    elif activation == "gelu":
        return F.gelu
    raise RuntimeError("activation should be relu/gelu, not {}".format(activation))


def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


class XformerEncoder(nn.Module):

    def __init__(self, cfg):
        super(XformerEncoder, self).__init__()
        encoder_layer = TransformerEncoderLayer(
            cfg.D_MODEL, cfg.D_K, cfg.D_V, cfg.D_FF, cfg.N_HEAD, cfg.MAX_REL_DIST,
            cfg.XFMR_ATTN_DROP, cfg.XFMR_RES_DROP, cfg.XFMR_FFN_DROP,
            cfg.HAS_MHA_BIAS, cfg.USE_NEG_DIST)
        self.xfmr_encoder = TransformerEncoder(encoder_layer, cfg.N_XFMR_LAYER)
        if cfg.MODEL_TYPE == "Xformer-abs":
            self.pos_encoder = LearnedPositionEncoding(cfg.D_PE, cfg.D_MODEL, cfg.PE_DROP, cfg.N_PE)

    def forward(self, x, mask):
        if hasattr(self, 'pos_encoder'):
            x = self.pos_encoder(x)
        x = self.xfmr_encoder(x, src_key_padding_mask=mask)
        return x


class LearnedPositionEncoding(nn.Embedding):

    def __init__(self, d_pe: int, d_model: int, p_drop: float = 0.1, n_pe: int = 100, is_fix: bool = True):
        super().__init__(n_pe, d_pe)
        self.is_add = (d_pe==d_model)
        # self.dropout = SharedDropout(p_drop)
        self.dropout = nn.Dropout(p=p_drop)

        if is_fix:
            nn.init.zeros_(self.weight)
            position = torch.arange(0, n_pe, dtype=torch.float).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_pe, 2).float() * (-math.log(10000.0)/d_pe))
            self.weight.data[:, 0::2] = torch.sin(position * div_term)
            self.weight.data[:, 1::2] = torch.cos(position * div_term)
            self.weight.requires_grad = False# False

    def forward(self, x):
        weight = self.weight.data.unsqueeze(1)
        if self.is_add:
            x += weight[:x.size(0),:]
        else:
            p_emb = weight[:x.size(0),:].expand(-1, x.size(1), -1)
            x = torch.cat((x, p_emb), dim=-1)
        return self.dropout(x)


class LearnedRelPositionEncoding(nn.Embedding):

    def __init__(self, ):
        dist_x = torch.arange(0, key_len).unsqueeze(0)
        dist_y = torch.arange(0, key_len).unsqueeze(1)
        distance = dist_x - dist_y
